{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d38eac",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark Cluster, read a local CSV and store it to Hadoop as partitioned parquet files. Is also shows how to ustilise MlLIB to train ML model by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326212e7",
   "metadata": {},
   "source": [
    "## 2. Connection to Spark Cluster\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31e35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-bank\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "# Get the SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "logger = logging.getLogger(\"py4j\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd5d0a",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "We will now load data from a local CSV and store it to Hadoop partitioned by column.\n",
    "Afterward you can access Hadoop UI to explore the saved parquet files.\n",
    "Access Hadoop UI on 'http://bigdata:9870' (Utilities -> Browse the files system )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81536ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e2056",
   "metadata": {},
   "source": [
    "### 3.1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed07f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_df = pd.read_csv('./data_bank/bank-additional-full.csv', sep=';')\n",
    "bank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcba2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "bank_spark_df = spark.createDataFrame(bank_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6cea96",
   "metadata": {},
   "source": [
    "### 3.2. Set number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9ed348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "number_of_partitions = bank_spark_df.rdd.getNumPartitions()\n",
    "print(\"Number of partitions:\", number_of_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b0cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_spark_df = bank_spark_df.repartition(10)\n",
    "bank_spark_df.cache()\n",
    "bank_spark_df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6d39542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 10\n"
     ]
    }
   ],
   "source": [
    "number_of_partitions = bank_spark_df.rdd.getNumPartitions()\n",
    "print(\"Number of partitions:\", number_of_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f198f5",
   "metadata": {},
   "source": [
    "### 3.3. Remove dots in the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12bca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Dataframe created with schema : \n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- campaign: long (nullable = true)\n",
      " |-- pdays: long (nullable = true)\n",
      " |-- previous: long (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp_var_rate: double (nullable = true)\n",
      " |-- cons_price_idx: double (nullable = true)\n",
      " |-- cons_conf_idx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nr_employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_spark_df = bank_spark_df.select([F.col(\"`\" + col + \"`\").alias(col.replace('.', '_')) \n",
    "                          for col in bank_spark_df.columns])\n",
    "print(\"Bank Dataframe created with schema : \")\n",
    "bank_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606ac52",
   "metadata": {},
   "source": [
    "### 3.4. Check for missing values\n",
    "Check for missing values and handle them appropriately (e.g. fill missing values with mean, median, or mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1f8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_spark_df = bank_spark_df.na.replace(\"unknown\", None)\n",
    "bank_spark_df = bank_spark_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de17673",
   "metadata": {},
   "source": [
    "### 3.5. One Hot Encoding Categorical data type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "categorical_columns = [bank_spark_df.columns[i] for i in list(range(1, 10)) + [14]]\n",
    "\n",
    "indexer_output_names = [name + \"__indexed\" for name in categorical_columns]\n",
    "encoder_output_names = [name + \"__vec\" for name in categorical_columns]\n",
    "\n",
    "indexer = StringIndexer(inputCols=categorical_columns, outputCols=indexer_output_names)\n",
    "onehoter = OneHotEncoder(inputCols=indexer_output_names, outputCols=encoder_output_names)\n",
    "\n",
    "indexed_df = indexer.fit(bank_spark_df).transform(bank_spark_df)\n",
    "one_hoted_df = onehoter.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "label_indexer = StringIndexer(inputCol='y', outputCol='label')\n",
    "one_hoted_df = label_indexer.fit(one_hoted_df).transform(one_hoted_df)\n",
    "\n",
    "one_hoted_df = one_hoted_df.select([F.col(col) for col in one_hoted_df.columns \n",
    "                                          if \"__vec\" in col or col not in categorical_columns and \"__indexed\" not in col])\n",
    "processed_spark_df = one_hoted_df.select([F.col(col) for col in one_hoted_df.columns if col != \"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5b922",
   "metadata": {},
   "source": [
    "### 3.6. Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5e53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols = processed_spark_df.columns\n",
    "\n",
    "seed = 42\n",
    "data_train, data_test = processed_spark_df.randomSplit([0.7, 0.3], seed=seed)\n",
    "# Use the VectorAssembler to combine the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "\n",
    "# Use the assembler to transform the dataset\n",
    "data_train = assembler.transform(data_train)\n",
    "data_test = assembler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547c39e",
   "metadata": {},
   "source": [
    "## 4. Model selection\n",
    "Choose an appropriate classification model (e.g. Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forest, Support Vector Machines, Neural Networks, etc.) based on the size and characteristics of the dataset and the problem you are trying to solve.\n",
    "It's a good idea to use cross-validation techniques to get an accurate estimate of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309fff2",
   "metadata": {},
   "source": [
    "### 4.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6df681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c7b66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression object\n",
    "logr = LogisticRegression(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc562ced",
   "metadata": {},
   "source": [
    "### 4.2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c16c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "146ba233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree classifier on the training data\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426cfd2",
   "metadata": {},
   "source": [
    "### 4.3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4d62087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bfb715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3c9aa",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "Train the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "202be924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the models on the training data\n",
    "lr_model = logr.fit(data_train)\n",
    "dt_model = dt.fit(data_train)\n",
    "rf_model = rf.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db838bc6",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "Evaluate the performance of the model on the testing data using appropriate metrics such as accuracy, precision, recall, F1-score, confusion matrix, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097c168",
   "metadata": {},
   "source": [
    "### 6.1. Use the model to predict the labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994c3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(data_test)\n",
    "dt_predictions = dt_model.transform(data_test)\n",
    "rf_predictions = rf_model.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3b011",
   "metadata": {},
   "source": [
    "### 6.2. Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d6befb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def prec_recall(predictions):\n",
    "    TN = predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = predictions.filter('prediction = 0 AND label = 1').count()\n",
    "    FP = predictions.filter('prediction = 1 AND label = 0').count()\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    try:\n",
    "        precision = TP / (TP + FP)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = TP / (TP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "lr_prec_recall = prec_recall(lr_predictions)\n",
    "dt_prec_recall = prec_recall(dt_predictions)\n",
    "rf_prec_recall = prec_recall(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "347124cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "precision = 1.00\n",
      "recall   = 1.00\n",
      "Decision tree classifier:\n",
      "precision = 1.00\n",
      "recall   = 1.00\n",
      "Random Forest classifier:\n",
      "precision = 1.00\n",
      "recall   = 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Linear Regression:')\n",
    "print('precision = {:.2f}\\nrecall   = {:.2f}'.format(*lr_prec_recall))\n",
    "print('Decision tree classifier:')\n",
    "print('precision = {:.2f}\\nrecall   = {:.2f}'.format(*dt_prec_recall))\n",
    "print('Random Forest classifier:')\n",
    "print('precision = {:.2f}\\nrecall   = {:.2f}'.format(*rf_prec_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79b993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
